# app.py
import streamlit as st
import pandas as pd
import numpy as np
from pathlib import Path
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
import unicodedata
import re

# ---------------- C·∫•u h√¨nh trang (Page Config) ----------------
st.set_page_config(
    page_title="Agoda Hotel Recommendation",
    page_icon="üè®",
    layout="wide"
)

# ---------------- C√°c h√†m t·∫£i v√† x·ª≠ l√Ω d·ªØ li·ªáu (v·ªõi cache) ----------------


# üîß Spark session (ƒë·∫∑t ·ªü ƒë·∫ßu file, ch·ªâ t·∫°o 1 l·∫ßn)
from pyspark.sql import SparkSession
import streamlit as st

# def get_spark():
#     if "spark" not in st.session_state:
#         st.session_state.spark = (
#             SparkSession.builder
#             .master("local[*]")
#             .appName("AgodaALSApp")
#             # ‚¨áÔ∏è b·∫Øt bu·ªôc: ƒë·ªçc local kh√¥ng ki·ªÉm checksum
#             .config("spark.hadoop.fs.file.impl", "org.apache.hadoop.fs.RawLocalFileSystem")
#             .config("spark.hadoop.fs.file.impl.disable.cache", "true")
#             .getOrCreate()
#         )
#         # √©p l·∫°i ·ªü HadoopConf (ph√≤ng tr∆∞·ªùng h·ª£p builder ch∆∞a √°p)
#         st.session_state.spark.sparkContext._jsc.hadoopConfiguration().set(
#             "fs.file.impl", "org.apache.hadoop.fs.RawLocalFileSystem"
#         )
#     return st.session_state.spark

# # N√∫t reset Spark (ƒë·ªÉ ƒë·∫£m b·∫£o config m·ªõi ƒë∆∞·ª£c d√πng)
# if st.sidebar.button("üîÑ Reset Spark"):
#     try:
#         st.session_state.spark.stop()
#     except Exception:
#         pass
#     st.session_state.pop("spark", None)
#     st.experimental_rerun()

# spark = get_spark()

# ==== Utils: chu·∫©n h√≥a/√°nh x·∫° t√™n c·ªôt ====
def normalize_col(s: str) -> str:
    return re.sub(r'[^a-z0-9]+', '_', str(s).lower()).strip('_')

def auto_rename_columns(df: pd.DataFrame, wanted: dict) -> pd.DataFrame:
    """
    wanted = {
      'Chu·∫©nMu·ªënC√≥': ['candidates...', '...']
    }
    -> ƒê·ªïi t√™n c√°c c·ªôt ƒëang c√≥ v·ªÅ ƒë√∫ng kh√≥a 'Chu·∫©nMu·ªënC√≥' n·∫øu t√¨m th·∫•y ·ª©ng vi√™n.
    """
    norm_map = {normalize_col(c): c for c in df.columns}
    rename_dict = {}
    for target, cands in wanted.items():
        cands_norm = [normalize_col(x) for x in cands + [target]]
        for k in cands_norm:
            if k in norm_map:
                rename_dict[norm_map[k]] = target
                break
    if rename_dict:
        df = df.rename(columns=rename_dict)
    return df

# ---------------- C√°c h√†m t·∫£i v√† x·ª≠ l√Ω d·ªØ li·ªáu (v·ªõi cache) ----------------
@st.cache_data
def load_main_data():
    """T·∫£i v√† chu·∫©n b·ªã d·ªØ li·ªáu ch√≠nh t·ª´ hotel_info.csv v√† hotel_comments.csv."""
    # Hotel info
    try:
        hotel_df = pd.read_csv("./data/hotel_info.csv", encoding="utf-8")
    except Exception:
        hotel_df = pd.read_csv("./data/hotel_info.csv", encoding="latin-1")
    hotel_df.columns = hotel_df.columns.str.strip()

    # √Ånh x·∫° c·ªôt th∆∞·ªùng g·∫∑p -> t√™n chu·∫©n d√πng trong app
    hotel_df = auto_rename_columns(
        hotel_df,
        {
            "Hotel_ID": ["hotel_id", "hotelid", "id_hotel", "property_id", "propertyid", "id"],
            "Hotel_Name": ["hotel_name", "name_hotel", "property_name", "name"],
            "Hotel_Address": ["hotel_address", "address", "addr"],
            "Hotel_Description": ["hotel_description", "description", "desc", "about", "overview", "summary"],
            "Image_URL": ["image_url", "image", "photo", "thumbnail"],
            "Hotel_Rank": ["hotel_rank", "rank", "stars", "rating_class", "star_rating"],
            "Total_Score": ["total_score", "score", "avg_score", "overall_score"]
        }
    )

    # N·∫øu thi·∫øu m√¥ t·∫£ th√¨ gh√©p t·∫°m t·ª´ t√™n + ƒë·ªãa ch·ªâ ƒë·ªÉ TF-IDF v·∫´n ch·∫°y
    if "Hotel_Description" not in hotel_df.columns:
        cols = [c for c in ["Hotel_Name", "Hotel_Address"] if c in hotel_df.columns]
        if cols:
            hotel_df["Hotel_Description"] = hotel_df[cols].astype(str).fillna("").agg(" ".join, axis=1)
        else:
            hotel_df["Hotel_Description"] = ""

    if "Total_Score" not in hotel_df.columns:
        hotel_df["Total_Score"] = np.random.uniform(7.5, 9.8, size=len(hotel_df)).round(1)

    # Comments / ratings
    try:
        comments_df = pd.read_csv("./data/hotel_comments.csv", encoding="utf-8")
    except Exception:
        comments_df = pd.read_csv("./data/hotel_comments.csv", encoding="latin-1")

    comments_df.columns = comments_df.columns.str.strip()
    comments_df = auto_rename_columns(
        comments_df,
        {
            "Reviewer_Name": ["reviewer_name", "reviewer", "user", "user_name", "username", "customer_name", "author", "name"],
            "Hotel_ID": ["hotel_id", "hotelid", "id_hotel", "property_id", "propertyid", "id"]
        }
    )

    # C·∫£nh b√°o d·ªÖ hi·ªÉu n·∫øu v·∫´n thi·∫øu c·ªôt b·∫Øt bu·ªôc
    missing = [c for c in ["Reviewer_Name", "Hotel_ID"] if c not in comments_df.columns]
    if missing:
        st.error(
            "Thi·∫øu c·ªôt b·∫Øt bu·ªôc trong file `hotel_comments.csv`: "
            + ", ".join(missing)
            + ".\nC√°c c·ªôt hi·ªán c√≥: "
            + ", ".join(list(comments_df.columns))
        )

    return hotel_df, comments_df

from pyspark.ml.recommendation import ALSModel
from pathlib import Path
import os, glob

def _delete_crc_under(dir_path: str):
    for fp in glob.glob(os.path.join(dir_path, "**", "*.crc"), recursive=True):
        try:
            os.remove(fp)
        except Exception:
            pass

@st.cache_resource
def load_als_model():
    # Tr√°nh nh·∫ßm sang b·∫£n ‚Äú- Copy‚Äù, v√† chu·∫©n h√≥a th√†nh file:/// URI
    model_dir = Path("./outputs/models/best_als_model").resolve()
    st.info(f"ƒêang load ALS model t·ª´: {model_dir}")  # ƒë·ªÉ b·∫°n nh√¨n ƒë√∫ng th∆∞ m·ª•c
    _delete_crc_under(str(model_dir))                 # ‚úÖ b·ªè CRC g√¢y l·ªách checksum
    uri = model_dir.as_uri()                          # vd: file:///C:/.../best_als_model
    try:
        return ALSModel.load(uri)
    except Exception as e:
        # √©p l·∫°i c·∫•u h√¨nh l·∫ßn n·ªØa r·ªìi th·ª≠ l·∫°i
        spark.sparkContext._jsc.hadoopConfiguration().set(
            "fs.file.impl", "org.apache.hadoop.fs.RawLocalFileSystem"
        )
        _delete_crc_under(str(model_dir))
        return ALSModel.load(uri)

@st.cache_resource
def create_tfidf_recommender(_hotel_df):
    """T·∫°o recommender d·ª±a tr√™n TF-IDF (ch·ªãu l·ªói c·ªôt m√¥ t·∫£ thi·∫øu)."""
    desc_col = "Hotel_Description" if "Hotel_Description" in _hotel_df.columns else None
    if not desc_col:
        # fallback an to√†n
        _hotel_df["__desc_fallback__"] = _hotel_df.astype(str).fillna("").agg(" ".join, axis=1)
        desc_col = "__desc_fallback__"
    corpus = _hotel_df[desc_col].astype(str).fillna("").apply(preprocess_text)
    tfidf = TfidfVectorizer(max_features=10000, ngram_range=(1, 2), min_df=2)
    tfidf_matrix = tfidf.fit_transform(corpus)
    cosine_sim = cosine_similarity(tfidf_matrix, tfidf_matrix)
    return tfidf, tfidf_matrix, cosine_sim

@st.cache_resource
def load_embeddings(model_name):
    """T·∫£i c√°c file embedding ƒë√£ t√≠nh s·∫µn (Doc2Vec, SBERT)."""
    # ƒê·ªïi 'output' -> 'outputs' cho ƒë√∫ng th∆∞ m·ª•c
    path = f"./outputs/models/{model_name}_emb.npy"
    try:
        embeddings = np.load(path)
        return embeddings
    except FileNotFoundError:
        st.warning(f"Kh√¥ng t√¨m th·∫•y embeddings t·∫°i: {path}")
        return None


def preprocess_text(text):
    """H√†m ti·ªÅn x·ª≠ l√Ω vƒÉn b·∫£n ti·∫øng Vi·ªát."""
    if not isinstance(text, str): text = str(text)
    text = unicodedata.normalize("NFC", text.lower())
    text = re.sub(r"[^\w\s]", " ", text)
    text = re.sub(r"\s+", " ", text).strip()
    return text

def get_content_recommendations(hotel_index, sim_matrix, df, top_n=10):
    """L·∫•y g·ª£i √Ω t·ª´ ma tr·∫≠n t∆∞∆°ng ƒë·ªìng ƒë√£ c√≥."""
    sim_scores = list(enumerate(sim_matrix[hotel_index]))
    sim_scores = sorted(sim_scores, key=lambda x: x[1], reverse=True)
    sim_scores = sim_scores[1:top_n+1]
    hotel_indices = [i[0] for i in sim_scores]
    return df.iloc[hotel_indices]

def display_recommendation_list(df_recommendations):
    """Hi·ªÉn th·ªã danh s√°ch kh√°ch s·∫°n ƒë∆∞·ª£c g·ª£i √Ω m·ªôt c√°ch ƒë·∫πp m·∫Øt."""
    if df_recommendations.empty:
        st.info("Kh√¥ng t√¨m th·∫•y g·ª£i √Ω n√†o.")
        return
    
    for index, row in df_recommendations.iterrows():
        with st.container(border=True):
            col1, col2 = st.columns([1, 4])
            with col1:
                st.image(row.get('Image_URL', 'https://i.imgur.com/uR3sYyP.jpeg'))
            with col2:
                st.subheader(row['Hotel_Name'])
                st.caption(f"üìç {row.get('Hotel_Address', 'N/A')}")
                st.write(f"‚≠ê **H·∫°ng:** {row.get('Hotel_Rank', 'N/A')} | üíØ **ƒêi·ªÉm:** {row.get('Total_Score', 'N/A')}")
                
# ---------------- Ch∆∞∆°ng tr√¨nh ch√≠nh ----------------

# --- T·∫£i v√† chu·∫©n b·ªã d·ªØ li·ªáu ---
hotel_df, comments_df = load_main_data()
tfidf_vectorizer, tfidf_matrix, tfidf_cosine_sim = create_tfidf_recommender(hotel_df)
d2v_embeddings = load_embeddings("d2v")
sbert_embeddings = load_embeddings("sbert")

# --- Giao di·ªán ---
st.title("AGODA Hotel Recommendation")
st.caption("Get tailored hotel suggestions with advanced filtering and multiple recommendation models.")

with st.sidebar:
    st.image("logo.png", width=100)
    st.header("Suggestions")
    page = st.radio(
        "",
        ('by hotel description', 'by hotel name', 'by rating review (ALS)')
    )
    st.markdown("---")
    st.header("About this project")
    st.info("ƒê·ªì √°n t·ªët nghi·ªáp ·ª©ng d·ª•ng c√°c thu·∫≠t to√°n g·ª£i √Ω v√†o b√†i to√°n th·ª±c t·∫ø tr√™n d·ªØ li·ªáu t·ª´ Agoda.")
    
# --- Hi·ªÉn th·ªã trang t∆∞∆°ng ·ª©ng ---
if page == 'by hotel description':
    st.header("T√¨m ki·∫øm theo m√¥ t·∫£")
    search_query = st.text_input("V√≠ d·ª•: kh√°ch s·∫°n m√°t, r·ªông, g·∫ßn bi·ªÉn, c√≥ tr·∫ª em", placeholder="Nh·∫≠p m√¥ t·∫£ c·ªßa b·∫°n ·ªü ƒë√¢y...")
    if search_query:
        with st.spinner("ƒêang t√¨m nh·ªØng kh√°ch s·∫°n ph√π h·ª£p nh·∫•t..."):
            query_vec = tfidf_vectorizer.transform([preprocess_text(search_query)])
            sim_scores = cosine_similarity(query_vec, tfidf_matrix).flatten()
            top_indices = sim_scores.argsort()[-10:][::-1]
            recommendations = hotel_df.iloc[top_indices]
        display_recommendation_list(recommendations)

elif page == 'by hotel name':
    st.header("T√¨m kh√°ch s·∫°n t∆∞∆°ng t·ª±")
    method = st.selectbox("Ch·ªçn m√¥ h√¨nh g·ª£i √Ω theo n·ªôi dung:", ["TF-IDF", "Doc2Vec", "SBERT"])
    selected_hotel_name = st.selectbox("Ch·ªçn m·ªôt kh√°ch s·∫°n b·∫°n ƒë√£ th√≠ch:", hotel_df['Hotel_Name'].unique())
    if selected_hotel_name:
        selected_hotel_index = hotel_df[hotel_df['Hotel_Name'] == selected_hotel_name].index[0]
        with st.spinner(f"ƒêang t√¨m c√°c kh√°ch s·∫°n t∆∞∆°ng t·ª± b·∫±ng {method}..."):
            recommendations = pd.DataFrame() # Kh·ªüi t·∫°o dataframe r·ªóng
            if method == "TF-IDF":
                recommendations = get_content_recommendations(selected_hotel_index, tfidf_cosine_sim, hotel_df)
            elif method == "Doc2Vec" and d2v_embeddings is not None:
                sim_matrix = cosine_similarity(d2v_embeddings)
                recommendations = get_content_recommendations(selected_hotel_index, sim_matrix, hotel_df)
            elif method == "SBERT" and sbert_embeddings is not None:
                sim_matrix = cosine_similarity(sbert_embeddings)
                recommendations = get_content_recommendations(selected_hotel_index, sim_matrix, hotel_df)
        st.subheader(f"Top 10 g·ª£i √Ω t∆∞∆°ng t·ª± '{selected_hotel_name}' theo {method}:")
        display_recommendation_list(recommendations)

elif page == 'by rating review (ALS)':
    st.header("G·ª£i √Ω cho kh√°ch h√†ng (M√¥ h√¨nh ALS)")
    st.info("T√≠nh nƒÉng n√†y s·ª≠ d·ª•ng Spark v√† c√≥ th·ªÉ m·∫•t m·ªôt ch√∫t th·ªùi gian ƒë·ªÉ kh·ªüi t·∫°o l·∫ßn ƒë·∫ßu.")
    
    user_list = comments_df['Reviewer_Name'].unique()
    selected_user = st.selectbox("Ch·ªçn m·ªôt kh√°ch h√†ng ƒë·ªÉ xem g·ª£i √Ω:", user_list)

    if st.button(f"L·∫•y g·ª£i √Ω cho {selected_user}", type="primary"):
        with st.spinner("ƒêang kh·ªüi t·∫°o Spark v√† t·∫£i m√¥ h√¨nh ALS..."):
            try:
                import findspark
                findspark.init()
                from pyspark.sql import SparkSession
                from pyspark.ml.recommendation import ALSModel

                spark = SparkSession.builder \
                    .appName("AgodaALSInference") \
                    .master("local[*]") \
                    .getOrCreate()

                # spark = SparkSession.builder.appName("ALSInference").master("local[*]").getOrCreate()
                
                user_map = pd.DataFrame({'Reviewer_Name': comments_df['Reviewer_Name'], 'userId': pd.factorize(comments_df['Reviewer_Name'])[0]}).drop_duplicates()
                selected_user_id = user_map[user_map['Reviewer_Name'] == selected_user]['userId'].iloc[0]
                
                model = ALSModel.load("./outputs/models/best_als_model")
                user_df = spark.createDataFrame([(selected_user_id,)], ["userId"])
                recs_spark = model.recommendForUserSubset(user_df, 10).first()

                if recs_spark and recs_spark['recommendations']:
                    recs_list = [(row['itemId'], row['rating']) for row in recs_spark['recommendations']]
                    recs_df = pd.DataFrame(recs_list, columns=['itemId', 'Score'])
                    
                    item_map = pd.DataFrame({'itemId': pd.factorize(comments_df['Hotel_ID'])[0], 'Hotel_ID': comments_df['Hotel_ID']}).drop_duplicates()
                    recs_df = recs_df.merge(item_map, on='itemId')
                    recs_df = recs_df.merge(hotel_df, on='Hotel_ID')
                    
                    st.subheader(f"Top 10 g·ª£i √Ω cho kh√°ch h√†ng '{selected_user}':")
                    display_recommendation_list(recs_df)
                else:
                    st.info(f"Kh√¥ng c√≥ g·ª£i √Ω n√†o cho kh√°ch h√†ng {selected_user}.")
                
                spark.stop()

            except Exception as e:
                st.error("C√≥ l·ªói x·∫£y ra khi ch·∫°y m√¥ h√¨nh ALS.")
                st.error(f"Chi ti·∫øt l·ªói: {e}")
                if 'spark' in locals() and spark.getActiveSession():
                    spark.stop()
